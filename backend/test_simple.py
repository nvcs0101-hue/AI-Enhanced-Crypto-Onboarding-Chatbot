"""Simple test of RAG pipeline"""
import os
os.environ['GOOGLE_API_KEY'] = 'REDACTED_GOOGLE_API_KEY'

from langchain_chroma import Chroma
from langchain_huggingface.embeddings import HuggingFaceEmbeddings
from langchain_google_genai import ChatGoogleGenerativeAI

print('ðŸ¤– Testing AI Crypto Chatbot with Gemini...')
print('=' * 60)

# Load vector store
embeddings = HuggingFaceEmbeddings(model_name="sentence-transformers/all-MiniLM-L6-v2")
vectorstore = Chroma(persist_directory='./chroma_db', embedding_function=embeddings)

# Search for relevant documents
docs = vectorstore.similarity_search("What is Bitcoin?", k=3)
print(f'\nðŸ“š Found {len(docs)} relevant documents')

# Initialize Gemini (updated model name)
llm = ChatGoogleGenerativeAI(
    model="gemini-1.5-flash",
    google_api_key=os.environ['GOOGLE_API_KEY'],
    temperature=0.3
)

# Create context and prompt
context = "\n\n".join([doc.page_content[:500] for doc in docs])
prompt = f"""You are a helpful crypto assistant.

Context: {context}

Question: What is Bitcoin?

Provide a clear, concise answer."""

print('\nðŸ’­ Generating response with Gemini 1.5 Flash...')
response = llm.invoke(prompt)

print('\nðŸ’¬ Answer:')
print(response.content)
print('\n' + '=' * 60)
print('âœ… SUCCESS! Your chatbot is working perfectly!')
print('ðŸŽ‰ Using Gemini 1.5 Flash (FREE - 60 requests/min)')
print('ðŸ’° Saving 70-80% on LLM costs vs OpenAI!')
